import pandas as pd
from selenium import webdriver
from selenium.webdriver.firefox.service import Service
from webdriver_manager.firefox import GeckoDriverManager
from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import os

from data_filtr import filter_data

def scrap_data_to_excel(num_pages, output_filename="otomoto_data.xlsx"):
    try:
        service = Service(GeckoDriverManager().install())
        options = webdriver.FirefoxOptions()
        options.add_argument("--headless")
        options.set_preference("general.useragent.override", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
        driver = webdriver.Firefox(service=service, options=options)
    except Exception as e:
        print("‚ùå B≈ÇƒÖd przy uruchamianiu przeglƒÖdarki:", e)
        return

    all_data = []

    for page_number in range(1, num_pages + 1):
        print(f"üåê Strona {page_number}...")
        try:
            url = f"https://www.otomoto.pl/osobowe?page={page_number}"
            driver.get(url)
            WebDriverWait(driver, 10).until(
                EC.presence_of_all_elements_located((By.TAG_NAME, "article"))
            )   
            soup = BeautifulSoup(driver.page_source, "html.parser")
            articles = soup.find_all("article", attrs={"data-id": True})
            #print(f"Znaleziono {len(articles)} artyku≈Ç√≥w na stronie.")

            if not articles:
                print("‚ö†Ô∏è Nie znaleziono og≈Çosze≈Ñ na stronie.")
                continue

            for article in articles:
                link = article.find("a", href=True)
                if link:
                    try:
                        link_url = link['href']
                        driver.get(link_url)

                        location_element = article.find("p", class_="ooa-oj1jk2")
                        location = location_element.get_text(strip=True) if location_element else "Nieznana lokalizacja"
                        
                        # Czekaj na za≈Çadowanie szczeg√≥≈Ç√≥w og≈Çoszenia
                        WebDriverWait(driver, 1).until(
                            EC.presence_of_element_located((By.CSS_SELECTOR, "div[data-testid='basic_information']"))
                        )
                        
                        article_soup = BeautifulSoup(driver.page_source, "html.parser")
                        
                        # Znajd≈∫ sekcjƒô szczeg√≥≈Ç√≥w og≈Çoszenia
                        details_section = article_soup.find_all("div", {"data-testid": True})
                        data_dict = {}
                        data_dict["Lokalizacja"] = location
                        # Pobierz cenƒô
                        price_element = article_soup.find("span", class_="offer-price__number")
                        if price_element:
                            price = price_element.get_text(strip=True)
                            data_dict["Cena"] = price

                        # Pobierz pozosta≈Çe szczeg√≥≈Çy
                        if details_section:
                            print("üîç Szczeg√≥≈Çy og≈Çoszenia znalezione.")
                            # Iteruj po wszystkich `div` z `data-testid`
                            for detail in details_section:
                                #key = detail.get("data-testid", "").strip()  # Pobierz nazwƒô szczeg√≥≈Çu z `data-testid`
                                
                                # Pobierz wszystkie tagi <p> w `div`
                                paragraphs = detail.find_all("p")
                                if len(paragraphs) > 1:  # Je≈õli sƒÖ dwa <p>, pierwszy to nazwa, drugi to warto≈õƒá
                                    key = paragraphs[0].get_text(strip=True)  # Pobierz nazwƒô z pierwszego <p>
                                    value = paragraphs[1].get_text(strip=True)  # Pobierz warto≈õƒá z drugiego <p>
                                else:  # Je≈õli brak <p>, sprawd≈∫ `aria-label`
                                    value = detail.get("aria-label", "").strip()
                                
                                if key and value:
                                    data_dict[key] = value

                        filtered_data = filter_data(data_dict)
                        all_data.append(filtered_data)
                        print(f"‚úÖ Zebrano dane: {filtered_data}")
                        print(filtered_data)
                    except Exception as e:
                        print(f"‚ö†Ô∏è B≈ÇƒÖd podczas przetwarzania og≈Çoszenia: {e}")
                        continue

        except Exception as e:
            print(f"‚ùå B≈ÇƒÖd na stronie {page_number}: {e}")
            continue

    driver.quit()

    if all_data:
        df = pd.DataFrame(all_data)
        current_folder = os.path.dirname(os.path.abspath(__file__))
        output_path = os.path.join(current_folder, output_filename)
        df.to_excel(output_path, index=False)
        print(f"üìÅ Dane zapisane do pliku: {output_filename}")
    else:
        print("‚ö†Ô∏è Nie zebrano ≈ºadnych danych. Plik nie zosta≈Ç zapisany.")


